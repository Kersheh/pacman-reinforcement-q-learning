#!/usr/bin/python

# Q-learning - Simplified Pacman
# Source: http://modelai.gettysburg.edu/2016/pyconsole/ex5/index.html

import sys, re, time, random, collections

## Game board with pellets and enemy ghost
class Environment:
    # environment constructor
    # var:      size - board size
    #           density - number of pellets
    def __init__(self, size, density):
        self.size = size
        self.density = density
        self.directions = [(0, 1), (1, 0), (0, -1), (-1, 0)] # left, down, right, up

    # initialize environment - randomize pacman, ghost, and pellets
    def initialize(self):
        locations = list()
        for r in range(1, self.size - 1):
            for c in range(1, self.size - 1):
                locations.append((r, c))
        
        random.shuffle(locations)
        self.pacman = locations.pop()
        
        self.pellets = set()
        for count in range(self.density):
            self.pellets.add(locations.pop())
            
        self.new_ghost()
        self.next_reward = 0
    
    # spawn ghost at one end of pacman's row or column randomly
    def new_ghost(self):
        (r, c) = self.pacman
        locations = [(r, 0), (0, c), (r, self.size - 1), (self.size - 1, c)]
        choice = random.choice(range(len(locations)))
        self.ghost = locations[choice]
        self.ghost_action = self.directions[choice]
    
    # print environment
    def display(self):
        for r in range(self.size):
            for c in range(self.size):
                if (r, c) == self.ghost:
                    print "G",
                elif (r, c) == self.pacman:
                    print "O",
                elif (r, c) in self.pellets:
                    print ".",
                elif r == 0 or r == self.size - 1:
                    print "X",
                elif c == 0 or c == self.size - 1:
                    print "X",
                else:
                    print " ",
            print
        print
    
    # return actions pacman can make in the environment
    def actions(self):
        # if self.is_over():
        #     return None
        # else:
        #     
        return self.directions

    # return whether the episode is done
    def is_over(self):
        if self.next_reward == -100:
            return True
        elif len(self.pellets) == 0:
            return True
        else:
            return False
    
    # return reward earned at last environment update
    def reward(self):
        return self.next_reward
        
    # update the environment based on agent's action
    def update(self, action):
        pacman = self.pacman
        ghost = self.ghost
        
        # pacman moves as chosen
        (r, c) = self.pacman
        (dr, dc) = action
        self.pacman = (r + dr, c + dc)
        
        # ghost moves in its direction
        (r, c) = self.ghost
        (dr, dc) = self.ghost_action
        self.ghost = (r + dr, c + dc)
        
        # ghost is replaced when it leaves
        (r, c) = self.ghost
        if r == 0 or r == self.size - 1:
            self.new_ghost()
        elif c == 0 or c == self.size - 1:
            self.new_ghost()
        
        (r, c) = self.pacman
        (gr, gc) = self.ghost
        
        # negative reward for hitting the ghost
        if self.pacman == self.ghost:
            self.next_reward = -100
        elif (pacman, ghost) == (self.ghost, self.pacman):
            self.next_reward = -100
        
        # negative reward for hitting a wall
        elif r == 0 or r == self.size - 1:
            self.next_reward = -100
        elif c == 0 or c == self.size - 1:
            self.next_reward = -100
        
        # positive reward for consuming a pellet
        elif self.pacman in self.pellets:
            self.next_reward = 10
            self.pellets.remove(self.pacman)
        else:
            self.next_reward = 0

    # return a dict of state attributes of the environment
    def state(self):
        s = dict()
        # s["pellets left"] = len(self.pellets) / float(self.density)
        adj = [[" " for i in range(9)] for j in range(9)]

        for x in range(-4, 5):
            for y in range(-4, 5):
                if (self.pacman[0] + x, self.pacman[1] + y) == self.pacman:
                    adj[x + 4][y + 4] = "O"
                elif (self.pacman[0] + x, self.pacman[1] + y) == self.ghost:
                    adj[x + 4][y + 4] = "G"
                elif (self.pacman[0] + x, self.pacman[1] + y) in self.pellets:
                    adj[x + 4][y + 4] = "."
                elif self.pacman[0] + x <= 0 or \
                     self.pacman[0] + x >= self.size - 1 or \
                     self.pacman[1] + y <= 0 or \
                     self.pacman[1] + y >= self.size - 1:
                    adj[x + 4][y + 4] = "X"
                else:
                    adj[x + 4][y + 4] = " "
                
        s["adjacent"] = adj
        
        # for x in range(0, 7):
        #     for y in range(0, 7):
        #         if s["adjacent"][x][y] is " ":
        #             print "_",
        #         else:
        #             print s["adjacent"][x][y],
        #     print
        # print

        return s

## Agent to learn actions within environment
class Agent:
    # agent constructor
    def __init__(self):
        self.w = collections.defaultdict(float) # each w((s, a)) starts at 0
        self.epsilon = 0.05 # exploration rate
        self.gamma = 0.99 # discount factor
        self.alpha = 0.01 # learning rate

    # return an action in a given state by the agent
    def choose(self, s, actions):
        p = random.random()
        if p < self.epsilon:
            return random.choice(actions)
        else:
            return self.policy(s, actions)

    # return the best action for a given state
    def policy(self, s, actions):
        max_value = max([self.Q(s, a) for a in actions])
        max_actions = [a for a in actions if self.Q(s, a) == max_value]
        return random.choice(max_actions)

    # return estimated Q-value of an action based on a given state
    def Q(self, s, a):
        return self.w[str((s, a))]
    
    # update weights based on observed step
    def update_weights(self, s, a, sp, r, actions):
        max_val = max([self.Q(sp, action) for action in actions])
        self.w[str((s, a))] = self.Q(s, a) + self.alpha * (r + self.gamma * max_val \
                                                    - self.Q(s, a))

# train an agent
def train_agent(env, agent, episodes=1000):
    for episode in range(episodes):
        if episode % 1000 == 0:
            print "Training {0}".format(episode)
        environment.initialize()
        while not environment.is_over():
            s = env.state()
            actions = env.actions()
            a = agent.choose(s, actions)
            env.update(a)
            
            sp = env.state()
            r = env.reward()
            actions = env.actions()
            agent.update_weights(s, a, sp, r, actions)

# execute trained agent
def run_agent(env, agent, refresh=0.1):
    env.initialize()
    env.display()
    while not env.is_over():
        s = env.state()
        actions = env.actions()
        a = agent.policy(s, actions)
        
        env.update(a)
        time.sleep(refresh)
        env.display()

def help():
    print "Q-learning Pacman"
    print "     commands: [t] train n    [a] age    [h] help"
    print "               [r] run        [q] quit"
    print "     train: optional n generations (default n=1000)"

# pacman q-learning interface
def ui(e, a):
    gen = 0
    cmd = ""
    cmds = [ "train", "t", "run", "r", "age", "a", "quit", "q", 
              "help", "h" ]
    help()
    print "     agent generation: 0"
    while cmd != "q" or cmd != "quit":
        cmd = raw_input("pacman> ")
        c = cmd.split(" ")
        if c[0] in cmds:
            if c[0] == "t" or c[0] == "train":
                if len(c) > 1:
                    try:
                        n = int(c[1])
                        if n:
                            gen += n
                            print "User: n={0} episodes.".format(n)
                            train_agent(e, a, n)
                    except:
                        print "Error: Invalid n episodes."
                else:
                    gen += 1000
                    print "Default: n=1000 episodes."
                    train_agent(e, a, episodes=1000)
            if c[0] == "r" or c[0] == "run":
                run_agent(e, a, refresh=0.09)
            if c[0] == "a" or c[0] == "age":
                print "Agent generation: {0}".format(gen)
            if c[0] == "h" or c[0] == "help":
                help()
            if c[0] == "q" or c[0] == "quit":
                sys.exit(0)
        else:
            print "Error: Invalid command."

if __name__ == "__main__":
    environment = Environment(20, 10)
    agent = Agent()

    ui(environment, agent)