#!/usr/bin/python

# Q-learning - Simplified Pacman
# Source: http://modelai.gettysburg.edu/2016/pyconsole/ex5/index.html

import os, sys, re, time, random, collections
import cPickle as pickle

## Game board with pellets and enemy ghost
class Environment:
    # environment constructor
    # var:      size - board size
    #           density - number of pellets
    def __init__(self, size, density):
        self.size = size
        self.density = density
        self.directions = [(0, 1), (1, 0), (0, -1), (-1, 0)] # left, down, right, up

    # initialize environment - randomize pacman, ghost, and pellets
    def initialize(self):
        locations = list()
        for x in range(1, self.size - 1):
            for y in range(1, self.size - 1):
                locations.append((x, y))
        
        random.shuffle(locations)
        self.pacman = locations.pop()
        
        self.pellets = set()
        for count in range(self.density):
            self.pellets.add(locations.pop())
            
        self.new_ghost()
        self.next_reward = 0
    
    # return 2D array of board
    def board(self):
        board = [[" " for i in range(self.size)] for j in range(self.size)]
        for x in range(self.size):
            if x == 0 or x == self.size - 1:
                for y in range(self.size):
                    board[x][y] = "X"
                    board[y][x] = "X"
        board[self.pacman[0]][self.pacman[1]] = "O"
        board[self.ghost[0]][self.ghost[1]] = "G"
        for pellet in self.pellets:
            board[pellet[0]][pellet[1]] = "."
        return board
                                
    # spawn ghost at one end of pacman's row or column randomly
    def new_ghost(self):
        (x, y) = self.pacman
        locations = [(x, 0), (0, y), (x, self.size - 1), (self.size - 1, y)]
        choice = random.choice(range(len(locations)))
        self.ghost = locations[choice]
        self.ghost_action = self.directions[choice]
    
    # print environment
    def display(self):
        for x in range(self.size):
            for y in range(self.size):
                if (x, y) == self.ghost:
                    print "G",
                elif (x, y) == self.pacman:
                    print "O",
                elif (x, y) in self.pellets:
                    print ".",
                elif x == 0 or x == self.size - 1:
                    print "X",
                elif y == 0 or y == self.size - 1:
                    print "X",
                else:
                    print " ",
            print
        print
    
    # return actions pacman can make in the environment
    def actions(self):
        # if self.is_over():
        #     return None
        # else:
        #     
        return self.directions

    # return whether the episode is done
    def is_over(self):
        if self.next_reward == -100:
            return True
        elif len(self.pellets) == 0:
            return True
        else:
            return False
    
    # return reward earned at last environment update
    def reward(self):
        return self.next_reward
        
    # update the environment based on agent's action
    def update(self, action):
        pacman = self.pacman
        ghost = self.ghost
        
        # pacman moves as chosen
        (x, y) = self.pacman
        (dx, dy) = action
        self.pacman = (x + dx, y + dy)
        
        # ghost moves in its direction
        (x, y) = self.ghost
        (dx, dy) = self.ghost_action
        self.ghost = (x + dx, y + dy)
        
        # ghost is replaced when it leaves
        (x, y) = self.ghost
        if x == 0 or x == self.size - 1:
            self.new_ghost()
        elif y == 0 or y == self.size - 1:
            self.new_ghost()
        
        (x, y) = self.pacman
        (gx, gy) = self.ghost
        
        # negative reward for hitting the ghost
        if self.pacman == self.ghost:
            self.next_reward = -100
        elif (pacman, ghost) == (self.ghost, self.pacman):
            self.next_reward = -100
        
        # negative reward for hitting a wall
        elif x == 0 or x == self.size - 1:
            self.next_reward = -100
        elif y == 0 or y == self.size - 1:
            self.next_reward = -100
        
        # positive reward for consuming a pellet
        elif self.pacman in self.pellets:
            self.next_reward = 10
            self.pellets.remove(self.pacman)
        else:
            self.next_reward = 0

    # return a state representing a window of view around Pacman's location
    def state(self, window):
        adj = [[" " for i in range(window * 2 + 1)] for j in range(window * 2 + 1)]

        for x in range(-window, window + 1):
            for y in range(-window, window + 1):
                if (self.pacman[0] + x, self.pacman[1] + y) == self.pacman:
                    adj[x + window][y + window] = "O"
                elif (self.pacman[0] + x, self.pacman[1] + y) == self.ghost:
                    adj[x + window][y + window] = "G"
                elif (self.pacman[0] + x, self.pacman[1] + y) in self.pellets:
                    adj[x + window][y + window] = "."
                elif self.pacman[0] + x <= 0 or \
                     self.pacman[0] + x >= self.size - 1 or \
                     self.pacman[1] + y <= 0 or \
                     self.pacman[1] + y >= self.size - 1:
                    adj[x + window][y + window] = "X"
                else:
                    adj[x + window][y + window] = " "
                
        return adj

## Agent to learn actions within environment
class Agent:
    # agent constructor
    # var:      w - dictionary of weights based on state, action pair
    #           epsilon - agent's exploration rate
    #           gamma - agent's Q discount factor
    #           alpha - agent's Q learning rate
    #           age - agent's age in generations/episodes
    def __init__(self, window=2):
        self.w = collections.defaultdict(float) # weights initialized at 0.0
        self.epsilon = 0.05
        self.gamma = 0.99
        self.alpha = 0.01
        self.age = 0
        self.window = window # agent's view window
                             # NOTE: changing this will conflict with different
                             # externally saved versions of the agent

    # debugging function formats state to string
    def _agent_state_to_string(self, state):
        state_list = eval(state)
        s = ""
        for x in range(0, self.window * 2 + 1):
            for y in range(0, self.window * 2 + 1):
                if state_list[0][x][y] is " ":
                    s += "_"
                else:
                    s += state_list[0][x][y]
            s += "\n"
        s += "Action: {0}\n".format(state_list[1])
        return s

    # debugging function exports agent dictionary
    def _agent_history(self):
        out = open("debug.txt", "w")
        s = ""
        for state in self.w:
            if self.w.get(state) != 0:
                s += "Q value: {0}\nState:\n{1}\n".format(self.w.get(state), \
                                                self._agent_state_to_string(state))
        out.write(s)
        out.close()

    # return an action in a given state by the agent
    def choose(self, s, actions):
        p = random.random()
        if p < self.epsilon:
            return random.choice(actions)
        else:
            return self.policy(s, actions)

    # return the best action for a given state
    def policy(self, s, actions):
        max_value = max([self.Q(s, a) for a in actions])
        max_actions = [a for a in actions if self.Q(s, a) == max_value]
        return random.choice(max_actions)

    # return estimated Q-value of an action based on a given state
    def Q(self, s, a):
        return self.w[repr((s, a))]
    
    # update weights based on observed step
    def update_weights(self, s, a, sp, r, actions):
        max_val = max([self.Q(sp, action) for action in actions])
        self.w[repr((s, a))] = self.Q(s, a) + self.alpha * \
                              (r + self.gamma * max_val - self.Q(s, a))

# train an agent
def train_agent(env, agent, episodes=1000):
    for episode in range(episodes):
        agent.age += 1
        if episode % 1000 == 0:
            n = episode + 1000
            if n > episodes:
                n = episodes
            print "Training {0} --> {1}".format(episode, n)
        environment.initialize()
        while not environment.is_over():
            s = env.state(agent.window)
            actions = env.actions()
            a = agent.choose(s, actions)
            env.update(a)
            
            sp = env.state(agent.window)
            r = env.reward()
            actions = env.actions()
            agent.update_weights(s, a, sp, r, actions)

# execute trained agent
def run_agent(env, agent, refresh=0.1):
    try:
        moves = 0
        env.initialize()
        env.display()
        while not env.is_over():
            s = env.state(agent.window)
            actions = env.actions()
            a = agent.policy(s, actions)
            
            env.update(a)
            time.sleep(refresh)
            env.display()
            moves += 1
        if env.pellets > 0:
            print "Pacman has died with {0} pellets remaining.".format(len(env.pellets))
        else:
            print "Pacman has found all the pellets!"
    except KeyboardInterrupt:
        print "\nRun interrupted."
    return moves

# display ui help
def help():
    print "Q-learning Pacman"
    print "     commands: [t] train n [n] new     [s] save f  [h] help    [d] debug"
    print "               [r] run     [i] info    [l] load f  [q] quit"
    print "         run: ctrl+c to gracefully interrupt instance"
    print "         train: train Pacman for n generations (default n=1000)"
    print "         load: import Pacman from f (filename)"
    print "         save: export Pacman to f (filename)"
    print "         debug: export agent's states to 'debug.txt'"

# display agent info
def info(agent):
    print "Pacman generation: {0}".format(agent.age)
    states = 0
    for w in agent.w:
        if agent.w.get(w) != 0:
            states += 1
    print "Total trained states: {0}".format(states)

# return new agent
def new():
    return Agent()

# return loaded agent from external file
def load(prev_a, f="default_save"):
    try:
        print "Loading Pacman from '" + str(f) + ".p'"
        a = pickle.load(open("saves/" + str(f) + ".p", "rb"))
    except:
        print "Error: File '" + f + ".p' not found."
        return prev_a
    return a

# save serialized agent to file
def save(agent, f="default_save"):
    path = "saves/" + str(f) + ".p"
    if os.path.isfile(path):
        print "test"
    if f != "default_save" and os.path.isfile(path):
        confirm = ""
        while True:
            confirm = raw_input("File '" + f + ".p' already exists. Overwrite? (y/n) ")
            if confirm.lower() == "y":
                print "Saving Pacman to '" + f + ".p'"
                pickle.dump(agent, open(path, "wb"))
                break
            elif confirm.lower() == "n":
                break
            else:
                print "Error: Invalid response (y/n)."
    else:
        print "Saving Pacman to '" + f + ".p'"
        pickle.dump(agent, open(path, "wb"))

# pacman q-learning interface
def ui(e, a):
    cmd = ""
    cmds = [ "train", "t", "run", "r", "info", "i", "quit", "q", 
              "help", "h", "load", "l", "save", "s", "new", "n", 
              "debug", "d" ]
    help()
    print "New Pacman - generation: 0"
    while cmd != "q" or cmd != "quit":
        cmd = raw_input("pacman> ")
        c = cmd.split(" ")
        if c[0] in cmds:
            if c[0] == "t" or c[0] == "train":
                if len(c) > 1:
                    try:
                        n = int(c[1])
                        if n:
                            print "User: n={0} episodes.".format(n)
                            train_agent(e, a, n)
                    except:
                        print "Error: Invalid n episodes."
                else:
                    print "Default: n=1000 episodes."
                    train_agent(e, a, episodes=1000)
            if c[0] == "r" or c[0] == "run":
                moves = run_agent(e, a, refresh=0.09)
                print "Pacman generation: {0}".format(a.age)
                print "Total moves: {0}".format(moves)
            if c[0] == "i" or c[0] == "info":
                info(a)
            if c[0] == "n" or c[0] == "new":
                a = new()
                print "New Pacman - generation: 0"
            if c[0] == "l" or c[0] == "load":
                if len(c) > 1:
                    a = load(a, c[1])
                else:
                    a = load(a)
            if c[0] == "s" or c[0] == "save":
                if len(c) > 1:
                    save(a, c[1])
                else:
                    save(a)
            if c[0] == "h" or c[0] == "help":
                help()
            if c[0] == "d" or c[0] == "debug":
                a._agent_history()
            if c[0] == "q" or c[0] == "quit":
                sys.exit(0)
        else:
            print "Error: Invalid command."

if __name__ == "__main__":
    environment = Environment(20, 10)
    agent = Agent()

    ui(environment, agent)