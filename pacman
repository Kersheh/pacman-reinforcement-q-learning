#!/usr/bin/python

# Q-learning - Simplified Pacman
# Source: http://modelai.gettysburg.edu/2016/pyconsole/ex5/index.html

import os, sys, re, time, random, collections
import cPickle as pickle

## Game board with pellets and enemy ghost
class Environment:
    # environment constructor
    # var:      size - board size
    #           density - number of pellets
    def __init__(self, size, density):
        self.size = size
        self.density = density
        self.directions = [(0, 1), (1, 0), (0, -1), (-1, 0)] # left, down, right, up

    # initialize environment - randomize pacman, ghost, and pellets
    def initialize(self):
        locations = list()
        for r in range(1, self.size - 1):
            for c in range(1, self.size - 1):
                locations.append((r, c))
        
        random.shuffle(locations)
        self.pacman = locations.pop()
        
        self.pellets = set()
        for count in range(self.density):
            self.pellets.add(locations.pop())
            
        self.new_ghost()
        self.next_reward = 0
    
    # spawn ghost at one end of pacman's row or column randomly
    def new_ghost(self):
        (r, c) = self.pacman
        locations = [(r, 0), (0, c), (r, self.size - 1), (self.size - 1, c)]
        choice = random.choice(range(len(locations)))
        self.ghost = locations[choice]
        self.ghost_action = self.directions[choice]
    
    # print environment
    def display(self):
        for r in range(self.size):
            for c in range(self.size):
                if (r, c) == self.ghost:
                    print "G",
                elif (r, c) == self.pacman:
                    print "O",
                elif (r, c) in self.pellets:
                    print ".",
                elif r == 0 or r == self.size - 1:
                    print "X",
                elif c == 0 or c == self.size - 1:
                    print "X",
                else:
                    print " ",
            print
        print
    
    # return actions pacman can make in the environment
    def actions(self):
        # if self.is_over():
        #     return None
        # else:
        #     
        return self.directions

    # return whether the episode is done
    def is_over(self):
        if self.next_reward == -100:
            return True
        elif len(self.pellets) == 0:
            return True
        else:
            return False
    
    # return reward earned at last environment update
    def reward(self):
        return self.next_reward
        
    # update the environment based on agent's action
    def update(self, action):
        pacman = self.pacman
        ghost = self.ghost
        
        # pacman moves as chosen
        (r, c) = self.pacman
        (dr, dc) = action
        self.pacman = (r + dr, c + dc)
        
        # ghost moves in its direction
        (r, c) = self.ghost
        (dr, dc) = self.ghost_action
        self.ghost = (r + dr, c + dc)
        
        # ghost is replaced when it leaves
        (r, c) = self.ghost
        if r == 0 or r == self.size - 1:
            self.new_ghost()
        elif c == 0 or c == self.size - 1:
            self.new_ghost()
        
        (r, c) = self.pacman
        (gr, gc) = self.ghost
        
        # negative reward for hitting the ghost
        if self.pacman == self.ghost:
            self.next_reward = -100
        elif (pacman, ghost) == (self.ghost, self.pacman):
            self.next_reward = -100
        
        # negative reward for hitting a wall
        elif r == 0 or r == self.size - 1:
            self.next_reward = -100
        elif c == 0 or c == self.size - 1:
            self.next_reward = -100
        
        # positive reward for consuming a pellet
        elif self.pacman in self.pellets:
            self.next_reward = 10
            self.pellets.remove(self.pacman)
        else:
            self.next_reward = 0

    # return a dict of state attributes of the environment
    def state(self):
        s = dict()
        # s["pellets left"] = len(self.pellets) / float(self.density)
        adj = [[" " for i in range(9)] for j in range(9)]

        for x in range(-4, 5):
            for y in range(-4, 5):
                if (self.pacman[0] + x, self.pacman[1] + y) == self.pacman:
                    adj[x + 4][y + 4] = "O"
                elif (self.pacman[0] + x, self.pacman[1] + y) == self.ghost:
                    adj[x + 4][y + 4] = "G"
                elif (self.pacman[0] + x, self.pacman[1] + y) in self.pellets:
                    adj[x + 4][y + 4] = "."
                elif self.pacman[0] + x <= 0 or \
                     self.pacman[0] + x >= self.size - 1 or \
                     self.pacman[1] + y <= 0 or \
                     self.pacman[1] + y >= self.size - 1:
                    adj[x + 4][y + 4] = "X"
                else:
                    adj[x + 4][y + 4] = " "
                
        s["adjacent"] = adj
        
        # for x in range(0, 9):
        #     for y in range(0, 9):
        #         if s["adjacent"][x][y] is " ":
        #             print "_",
        #         else:
        #             print s["adjacent"][x][y],
        #     print
        # print

        return s

## Agent to learn actions within environment
class Agent:
    # agent constructor
    # var:      w - dictionary of weights based on state, action pair
    #           epsilon - agent's exploration rate
    #           gamma - agent's Q discount factor
    #           alpha - agent's Q learning rate
    #           age - agent's age in generations/episodes
    def __init__(self):
        self.w = collections.defaultdict(float) # weights initialized at 0.0
        self.epsilon = 0.05
        self.gamma = 0.99
        self.alpha = 0.01
        self.age = 0

    # return an action in a given state by the agent
    def choose(self, s, actions):
        p = random.random()
        if p < self.epsilon:
            return random.choice(actions)
        else:
            return self.policy(s, actions)

    # return the best action for a given state
    def policy(self, s, actions):
        max_value = max([self.Q(s, a) for a in actions])
        max_actions = [a for a in actions if self.Q(s, a) == max_value]
        return random.choice(max_actions)

    # return estimated Q-value of an action based on a given state
    def Q(self, s, a):
        return self.w[str((s, a))]
    
    # update weights based on observed step
    def update_weights(self, s, a, sp, r, actions):
        max_val = max([self.Q(sp, action) for action in actions])
        self.w[str((s, a))] = self.Q(s, a) + self.alpha * \
                              (r + self.gamma * max_val - self.Q(s, a))

# train an agent
def train_agent(env, agent, episodes=1000):
    for episode in range(episodes):
        agent.age += 1
        if episode % 1000 == 0:
            n = episode + 1000
            if n > episodes:
                n = episodes
            print "Training {0} --> {1}".format(episode, n)
        environment.initialize()
        while not environment.is_over():
            s = env.state()
            actions = env.actions()
            a = agent.choose(s, actions)
            env.update(a)
            
            sp = env.state()
            r = env.reward()
            actions = env.actions()
            agent.update_weights(s, a, sp, r, actions)

# execute trained agent
def run_agent(env, agent, refresh=0.1):
    env.initialize()
    env.display()
    while not env.is_over():
        s = env.state()
        actions = env.actions()
        a = agent.policy(s, actions)
        
        env.update(a)
        time.sleep(refresh)
        env.display()
    if env.pellets > 0:
        print "Pacman has died with {0} pellets remaining.".format(len(env.pellets))
    else:
        print "Pacman has found all the pellets!"

# display ui help
def help():
    print "Q-learning Pacman"
    print "     commands: [t] train n   [n] new     [s] save f  [h] help"
    print "               [r] run       [i] info    [l] load f  [q] quit"
    print "         train: optional n generations (default n=1000)"
    print "         load: import Pacman from f (filename)"
    print "         save: export Pacman to f (filename)"

# return new agent
def new():
    return Agent()

# return loaded agent from external file
def load(prev_a, f="default_save"):
    try:
        print "Loading Pacman from '" + str(f) + ".p'"
        a = pickle.load(open("saves/" + str(f) + ".p", "rb"))
    except:
        print "Error: File '" + f + ".p' not found."
        return prev_a
    return a

# save serialized agent to file
def save(agent, f="default_save"):
    path = "saves/" + str(f) + ".p"
    if os.path.isfile(path):
        print "test"
    if f != "default_save" and os.path.isfile(path):
        confirm = ""
        while True:
            confirm = raw_input("File '" + f + ".p' already exists. Overwrite? (y/n) ")
            if confirm.lower() == "y":
                print "Saving Pacman to '" + f + ".p'"
                pickle.dump(agent, open(path, "wb"))
                break
            elif confirm.lower() == "n":
                break
            else:
                print "Error: Invalid response (y/n)."
    else:
        print "Saving Pacman to '" + f + ".p'"
        pickle.dump(agent, open(path, "wb"))

# pacman q-learning interface
def ui(e, a):
    cmd = ""
    cmds = [ "train", "t", "run", "r", "info", "i", "quit", "q", 
              "help", "h", "load", "l", "save", "s", "new", "n" ]
    help()
    print "New Pacman - generation: 0"
    while cmd != "q" or cmd != "quit":
        cmd = raw_input("pacman> ")
        c = cmd.split(" ")
        if c[0] in cmds:
            if c[0] == "t" or c[0] == "train":
                if len(c) > 1:
                    try:
                        n = int(c[1])
                        if n:
                            print "User: n={0} episodes.".format(n)
                            train_agent(e, a, n)
                    except:
                        print "Error: Invalid n episodes."
                else:
                    print "Default: n=1000 episodes."
                    train_agent(e, a, episodes=1000)
            if c[0] == "r" or c[0] == "run":
                run_agent(e, a, refresh=0.09)
                print "Pacman generation: {0}".format(a.age)
            if c[0] == "info" or c[0] == "i":
                print "Pacman generation: {0}".format(a.age)
            if c[0] == "new" or c[0] == "n":
                a = new()
                print "New Pacman - generation: 0"
            if c[0] == "l" or c[0] == "load":
                if len(c) > 1:
                    a = load(a, c[1])
                else:
                    a = load(a)
            if c[0] == "s" or c[0] == "save":
                if len(c) > 1:
                    save(a, c[1])
                else:
                    save(a)
            if c[0] == "h" or c[0] == "help":
                help()
            if c[0] == "q" or c[0] == "quit":
                sys.exit(0)
        else:
            print "Error: Invalid command."

if __name__ == "__main__":
    environment = Environment(20, 10)
    agent = Agent()

    ui(environment, agent)